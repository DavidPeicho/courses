{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> TP 3 : Apprentissage d'un mélange - Algorithme EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "%matplotlib inline \n",
    "\n",
    "from mnist import load_mnist\n",
    "import numpy as np\n",
    "import visualize as vz\n",
    "import random as rd\n",
    "\n",
    "train_data, train_labels = load_mnist(dataset='training', path='./')\n",
    "test_data, test_labels = load_mnist(dataset='testing', path='./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation de `train_data` et `test_data` en vecteur colonne pour chaque exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.reshape(train_data, (60000, 28 * 28)).T\n",
    "test_data  = np.reshape(test_data,  (10000, 28 * 28)).T\n",
    "train_classes = [train_data[:, np.where(train_labels == i)[0]]\n",
    "                        for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binarisation des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAACbCAYAAABVqOFYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQAgNGSDEPyp7sxY00FSvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdebe34a390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "binarize = lambda x : x < 128 / 255\n",
    "vfunc = np.vectorize(binarize)\n",
    "trainb_data = vfunc(train_data)\n",
    "testb_data = vfunc(test_data)\n",
    "trainb_classes = [trainb_data[:, np.where(train_labels == i)[0]]\n",
    "                        for i in range(10)]\n",
    "\n",
    "vz.plotGroupImages(trainb_data[:, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 : Théorie\n",
    "\n",
    "A partir de ce qui a été vu en cours, déduire les étapes de l'algorithme EM pour l'apprentissage des paramètres d'un mélange de loi de Bernoulli. Donner les formules de mise à jour des différents paramètres du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "L'algorithme EM fonctionne suivant deux étapes :\n",
    "* L'étape appelée \"expectation\", qui consiste à calculer tous les poids de tous les points pour toutes les composantes.\n",
    "* L'étape appelée \"maximization\", qui vise à recalculer les moyennes pour chaque composante, en modifiant les poids de chaque composante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pose N correspondant aux nombres d'éléments dans le dataset {$X_1, \\ldots, X_n$}, K correspondant aux nombres de composantes souhaitées,\n",
    "$w_i$ le poid associé à la i-ème composante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Etape 1 : Initialisation des paramètres**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialisation des moyennes de chaque composante.\n",
    "* Initialisation des poids, tels que $\\sum_{i=1}^{K}{w_i} = \\sum_{i=1}^K{P(i)} = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Etape 2 : Expectation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nous savons que la vraisemblance $P(X_i | \\mu_k) = \\mu_k^{X_i} (1 − \\mu_k)^{N − X_i}$ en utilisant une distribution de Bernoulli.\n",
    "* On calcule l'a posteriori : $P(k|X_i) = \\frac{w_k P(X_i|\\mu_k)}{\\sum_{j=1}^{K}{w_j P(X_j|\\mu_k)}} = \\frac{w_k (\\mu_k^{X_i} (1 − \\mu_k)^{N − X_i})}{\\sum_{j=1}^{K}{w_j (\\mu_k^{X_j}(1 − \\mu_k)^{N − X_j})}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Etape 3 : Maximization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On calcule de nouveaux poids : $P(k) = w_k = \\frac{\\sum_{i=1}^{N}{P(k|X_i)}}{N}$\n",
    "* On calcule les nouvelles moyennes : $\\mu_k = \\frac{\\sum_{i=1}^{N}{P(k|X_i)X_i}}{\\sum_{i=1}^{N}{P(k|X_i)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calcul de la probabilité de Bernoulli**\n",
    "\n",
    "TODOOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 : Implémentation\n",
    "\n",
    "On vous demandera de tester différentes valeurs pour le nombre de composantes du mélange ainsi que différentes initialisations du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Implémenter cet algorithme et faire l'apprentissage sur les données binaires TRAINB obtenues précedemment. Attention à la manipulation des valeurs de probabilités (entre 0 et 1) qui deviennent rapidement nulles à cause de la précision des chiffres flottants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmBernoulli:\n",
    "    def __init__(self, data, nb_components):\n",
    "        self.data = data\n",
    "        self.nb_components = nb_components\n",
    "\n",
    "    def _init_center(self):\n",
    "        self.center = np.zeros((self.data.shape[0], self.nb_components))\n",
    "        minimages = self.data.shape[1] // 4\n",
    "        for i in range(self.nb_components):\n",
    "            mini = 0\n",
    "            maxi = rd.randint(minimages, self.data.shape[1])\n",
    "            self.center[:, i] = np.mean(self.data[:, mini:maxi], axis=1)\n",
    "\n",
    "    def computeEM(self):\n",
    "        delta = 1e-2\n",
    "        # P(k)\n",
    "        self.W = np.repeat(1 / self.nb_components, self.nb_components)\n",
    "        self._init_center()\n",
    "        \n",
    "        while True:\n",
    "            self._expectationStep()\n",
    "            newW, newCenter = self._maximizationStep()\n",
    "            diff = np.sum(np.abs(newW - self.W))\n",
    "            #print(diff)\n",
    "            # Cas d'arrêt sur la vraisemblance à faire !!!\n",
    "            if diff < delta:\n",
    "                print(\"EM done!\")\n",
    "                return newW, newCenter\n",
    "            self.W = newW\n",
    "            self.center = newCenter\n",
    "\n",
    "    def _bernoulli(self, x, center):\n",
    "        # proba is P(X1, ..., Xn)\n",
    "        proba = np.zeros(x.shape)\n",
    "        # Retrieve indices of black and white pixels.\n",
    "        indices = [np.array(np.where(x == i)) for i in range(2)]\n",
    "        proba[indices[0]] = 1 - center[indices[0]]\n",
    "        proba[indices[1]] = center[indices[1]]\n",
    "        prod = np.prod(proba)\n",
    "\n",
    "        # This lines allow to fix NaN problems,\n",
    "        # by replacing them by smallest epsilon\n",
    "        if np.isnan(prod) or prod <= 0:\n",
    "            return np.finfo(prod.dtype).tiny\n",
    "        return prod\n",
    "\n",
    "    def _expectationStep(self):\n",
    "        N = self.data.shape[1]\n",
    "        K = self.nb_components\n",
    "        self.tabl = np.zeros((N, K))\n",
    "\n",
    "        for n in range(N):\n",
    "            for k in range(K):\n",
    "                # Probability for the image to be from K class.\n",
    "                self.tabl[n, k] = np.log(self._bernoulli(self.data[:, n], self.center[:, k])) + np.log(self.W[k])\n",
    "                \n",
    "            maxValue = np.amax(self.tabl[n, :])\n",
    "            normTabl = self.tabl[n, :] - maxValue\n",
    "            tablSum = np.sum(np.exp(normTabl))\n",
    "            self.tabl[n, :] = np.exp(normTabl - np.log(tablSum))\n",
    "\n",
    "    def _maximizationStep(self):\n",
    "        N = self.tabl.shape[0]\n",
    "        tsum = np.sum(self.tabl, axis=0)\n",
    "        W =  tsum / N\n",
    "        center = np.dot(self.data, self.tabl) / tsum\n",
    "        return W, center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#emb = EmBernoulli(trainb_classes[3], 4)\n",
    "#W, center = emb.computeEM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Afficher les moyennes des différentes distributions de Bernoulli ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vz.plotGroupImages(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** En utilisant uniquement 10 composantes, est-il possible d'avoir un centre par classe ? Justifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non, il n'est pas possible d'utiliser un centre par classe, car, comme affiché ci-dessus, les centres obtenus peuvent être \"proches\", ne permettant pas de les distinguer les uns entre les autres. Par exemple, le \"7\" et le \"1\" ont tendance à se ressembler, tout comme le \"3\" et le \"8\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#emb = EmBernoulli(trainb_data, 10)\n",
    "#W, center = emb.computeEM()\n",
    "#vz.plotGroupImages(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Tester un classifieur bayésien en utilisant un mélange pour chaque classe ? Quel est le taux de reconnaissance obtenu (tester 1, 2, 4, et 8 composantes par classe) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BernouilliClassifier:    \n",
    "    def train(self, train_data, train_labels, nb_comp):\n",
    "        tinier = np.vectorize(lambda x : np.finfo(centers.dtype).tiny if x <= 0 else x)\n",
    "        \n",
    "        self.nb_classes = np.unique(train_labels).shape[0]\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.nb_comp = nb_comp\n",
    "        self.centers_by_class = np.zeros((self.nb_classes, train_data.shape[0], self.nb_comp))\n",
    "        self.neg_centers_by_class = np.zeros((self.nb_classes, train_data.shape[0], self.nb_comp))\n",
    "        \n",
    "        for i in range(self.nb_classes):\n",
    "            emb = EmBernoulli(self.train_data[:, np.where(train_labels == i)[0]], self.nb_comp)\n",
    "            _, centers = emb.computeEM()\n",
    "            self.centers_by_class[i] = np.log(centers)\n",
    "            invcenters = tinier(1 - centers)\n",
    "            self.neg_centers_by_class[i] = np.log(invcenters)\n",
    "            \n",
    "    def process(self, data):\n",
    "        nb_samples = data.shape[1]\n",
    "        result = np.zeros((data.shape[1]))\n",
    "        for i in range(nb_samples):\n",
    "            prob = np.zeros((self.nb_classes, self.nb_comp))\n",
    "            for k in range(self.nb_classes):\n",
    "                x = data[:, i]\n",
    "                prob[k] = x.T.dot(self.centers_by_class[k]) + (1 - x).T.dot(self.neg_centers_by_class[k])\n",
    "            result[i] = np.unravel_index(np.argmax(prob), (self.nb_classes, self.nb_comp))[0]\n",
    "        return result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nb_components = [1, 2, 4, 8]\n",
    "#bc = BernouilliClassifier()\n",
    "#for i in nb_components:\n",
    "#    bc.train(trainb_data, train_labels, i)\n",
    "#    # Checks performance\n",
    "#    out = bc.process(testb_data)\n",
    "#    score = (out == test_labels).sum() / test_labels.shape[0]\n",
    "#    print(\"Perf with\", i, \"components\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 : Comparaison avec un GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Comparer les résultats obtenus avec le cas d'un mélange de gaussiennes sur les données brutes TRAIN et TEST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On a $N(X; \\mu, \\Theta) = \\frac{1}{\\sqrt{(2\\pi)^D|\\Theta|}} exp(-0.5(X - \\mu)^T\\Theta^{-1}(X - \\mu))$\n",
    "* La vraisemblance : $P(k|X) = \\frac{w_k N(X;\\mu_k,\\Theta_k)}{\\sum_{j=1}^{N}{w_j N(X,\\mu_j,\\Theta_j)}}$\n",
    "* $P(k) = \\frac{\\sum_{i=1}^{N}{P(k|X_i)}}{N}$\n",
    "* $\\mu_k = \\frac{\\sum_{i=1}^{N}{P(k|X_i)X_i}}{\\sum_{i=1}^{N}{P(k|X_i)}}$\n",
    "* $\\Theta_k = \\frac{\\sum_{i=1}^{N}{P(k|X_i)(X_i - \\mu_k)(X_i - \\mu_k)^T}}{\\sum_{i=1}^{N}{P(k|X_i)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Envisager le cas de gaussienne avec des matrices de covariance diagonale, ensuite tester la version avec matrices de covariance complètes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tinier(x):\n",
    "    return np.finfo(np.float64).tiny if np.isnan(x) or x <= 0 else x\n",
    "\n",
    "class EmGaussian:\n",
    "    l2pi = np.log(2 * np.pi)   \n",
    "\n",
    "    def _compute_cov_det_inv(self, i):\n",
    "        self.cov[i] += self.diag\n",
    "        #print(self.cov[i])\n",
    "        self.inv_cov[i] = np.linalg.pinv(self.cov[i])\n",
    "        self.det_cov[i] = np.linalg.det(self.cov[i])\n",
    "        #print(self.det_cov[i])\n",
    "\n",
    "    def _init_data(self):\n",
    "        # P(k)\n",
    "        D, N = self.data.shape\n",
    "        self.W = np.repeat(1 / self.nb_components, self.nb_components)\n",
    "        self.center = np.zeros((D, self.nb_components))\n",
    "        self.cov = np.zeros((self.nb_components, D, D))\n",
    "        self.inv_cov = np.zeros((self.nb_components, D, D))\n",
    "        self.det_cov = np.zeros(self.nb_components)\n",
    "        minimages = N // 4\n",
    "\n",
    "        for i in range(self.nb_components):\n",
    "            mini = 0\n",
    "            maxi = rd.randint(minimages, self.data.shape[1])\n",
    "            self.center[:, i] = np.mean(self.data[:, mini:maxi], axis=1)\n",
    "            self.cov[i] = np.eye(D) * self.nb_components #np.cov(self.data)\n",
    "            self._compute_cov_det_inv(i)\n",
    "\n",
    "\n",
    "    def __init__(self, data, nb_components, diag_version, delta=1e-2):\n",
    "        self.data = data\n",
    "        self.nb_components = nb_components\n",
    "        self.delta = delta\n",
    "        self.diag_version = diag_version\n",
    "        self.diag = 1e-3 * np.eye(self.data.shape[0])\n",
    "        \n",
    "    def computeEM(self):\n",
    "        self._init_data()\n",
    "\n",
    "        for i in range(10):\n",
    "        #while True:\n",
    "            self._expectationStep()\n",
    "            old_center = self.center\n",
    "            self._maximizationStep()\n",
    "            diff = np.sum(np.abs(self.center - old_center))\n",
    "            print(\"Diff :\", diff)\n",
    "            #if diff < self.delta:\n",
    "        return self.W, self.center, self.cov\n",
    "\n",
    "    def _gaussian(self, n, k):\n",
    "        # Here, we have to use the logarithm\n",
    "        # in order to store the value in a double\n",
    "        D = self.data.shape[0]\n",
    "        xcentered = self.data[:, n] - self.center[:, k]\n",
    "\n",
    "        a = xcentered.T.dot(self.inv_cov[k]).dot(xcentered)\n",
    "        ldet = np.log(self.det_cov[k])\n",
    "        res = -0.5 * (D * self.l2pi + ldet + a)\n",
    "        if np.isnan(res) or res <= 0:\n",
    "            return np.finfo(res.dtype).tiny\n",
    "        return res\n",
    "\n",
    "    def _expectationStep(self):\n",
    "        D, N = self.data.shape # Number of data, pixels.\n",
    "        K = self.W.shape[0] # Number of components.\n",
    "        self.tabl = np.zeros((N, K))\n",
    "        # Likelihood\n",
    "        for n in range(N):\n",
    "            for k in range(K):\n",
    "                # Probability for the image to be from K class.\n",
    "                self.tabl[n, k] = self._gaussian(n, k) + np.log(self.W[k])\n",
    "            \n",
    "            max_value = np.amax(self.tabl[n, :])\n",
    "            norm_tabl = self.tabl[n, :] - max_value\n",
    "            tabl_sum = np.sum(np.exp(norm_tabl))\n",
    "            #print(tabl_sum)\n",
    "            #print(self.tabl[n, :])\n",
    "            self.tabl[n, :] = np.exp(norm_tabl - np.log(tabl_sum))\n",
    "\n",
    "\n",
    "    def _maximizationStep(self):\n",
    "        N, K = self.tabl.shape # Number of data, components.\n",
    "        D = self.data.shape[0] # Number of pixels.\n",
    "        # print(self.tabl)\n",
    "        t_sum = np.sum(self.tabl, axis=0)\n",
    "\n",
    "        # Weights\n",
    "        self.W =  t_sum / N\n",
    "        # Means\n",
    "        self.center = self.data.dot(self.tabl) / t_sum\n",
    "        \n",
    "        # Covariances\n",
    "        for k in range(K):\n",
    "            mu_k = np.tile(self.center[:, k].reshape((-1, 1)), (1, N))\n",
    "            x_centered = self.data - mu_k\n",
    "            cov = np.zeros((D, D))\n",
    "            for i in range(N):\n",
    "                cov += self.tabl[i, k] * x_centered[:, i].dot(x_centered[:, i].T)\n",
    "            #print(cov)\n",
    "            #print(t_sum[k])\n",
    "            self.cov[k] = cov / t_sum[k]\n",
    "            self._compute_cov_det_inv(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff : 2.25208400613\n",
      "Diff : 0.0\n",
      "Diff : 0.0\n",
      "Diff : 0.0\n",
      "Diff : 0.0\n",
      "Diff : 0.0\n",
      "Diff : 0.0\n",
      "Diff : 0.0\n",
      "Diff : 0.0\n",
      "Diff : 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABeCAYAAAAUjW5fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdeb93a4518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(train_classes[3].T)\n",
    "train_1_pca = pca.transform(train_classes[3].T).T\n",
    "\n",
    "emg = EmGaussian(train_1_pca, 4, False)\n",
    "W, center, cov = emg.computeEM()\n",
    "invpcacenter = pca.inverse_transform(center.T).T\n",
    "vz.plotGroupImages(invpcacenter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Attention à la dégénérescence de l'algorithme EM (matrice de covariance non inversible). Il faut voir comment gérer ce problème ainsi que l'initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Proposer la meilleure solution pour avoir le plus haut taux de reconnaissance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indications\n",
    "\n",
    "For this TP, I will first make the implementation of each algorithm, and check the results with some english famous words and less famous word, in order to have a rough idea of whether it works or not.\n",
    "\n",
    "After that, I will simply compute the RMSE and ARE and draw some graphics in order to see how far I was from the real result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_scikit_data = fetch_20newsgroups(subset='train')\n",
    "\n",
    "import re\n",
    "import mmh3\n",
    "import random\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents found: 11314\n",
      "Example with document 0...\n",
      "\n",
      "-------- DOCUMENT 0 --------\n",
      "\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ END DOCUMENT 0 ------\n",
      "False\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# We only need to save a list of all the 20newsgroup documents\n",
    "newsgroups = newsgroups_scikit_data.data\n",
    "\n",
    "# We make sure the loading is OK.\n",
    "print('Number of documents found: {}'.format(len(newsgroups)))\n",
    "print('Example with document 0...\\n')\n",
    "print('-------- DOCUMENT 0 --------\\n')\n",
    "print(newsgroups[0])\n",
    "print('------ END DOCUMENT 0 ------')\n",
    "\n",
    "# Here, we have a lot of documents. The processing will be really slow if we go through\n",
    "# the all newsgroups. We will split the dataset in a smaller piece.\n",
    "\n",
    "NB_DOCUMENTS = 500\n",
    "\n",
    "if len(newsgroups) > NB_DOCUMENTS:\n",
    "    newsgroups = newsgroups[:NB_DOCUMENTS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the newsgroups to have hashable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "# We preprocesses the Newsgroups by first splitting the document\n",
    "# into a set of hashable word, and then by applying a normalizer.\n",
    "for doc in newsgroups:\n",
    "    words = re.split(\"[.,; ><()\\t\\n\\!?:]+\", doc)\n",
    "    for i in range (0, len(words)):\n",
    "        word = words[i]\n",
    "        unicode_str = unicodedata.normalize(\"NFKD\", word).encode(\"ascii\", \"ignore\")\n",
    "        words[i] = unicode_str.lower().decode(\"utf-8\")\n",
    "    documents.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the real count with a map (used for error checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perfect_count(documents):\n",
    "    \"\"\"This function compute the real count of each word.\n",
    "    \n",
    "    It does so by using a map and incrementing the counter each time\n",
    "    the word is seen. It is quite uneffective regarding memory consumption,\n",
    "    but will allows us to check the erros made by our algorithms.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    perfect_count = {}\n",
    "    \n",
    "    for doc in documents:\n",
    "        for word in doc:\n",
    "            if word in perfect_count:\n",
    "                perfect_count[word] = perfect_count[word] + 1\n",
    "            else:\n",
    "                perfect_count[word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the RMSE and ARE error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_profiling(d_batch, w_batch, perfect_count, documents, type=\"RMSE\"):\n",
    "    \"\"\"Computes the error between the real count of each word and our approximation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        \n",
    "    d_batch:\n",
    "        Provided batch of `d' values, e.g: [2, 4, 8].\n",
    "        \n",
    "    w_batch:\n",
    "        Provided batch of `w' values, e.g: [75, 100, 500].\n",
    "        \n",
    "    perfect_count:\n",
    "        Map containing the exact count of each word.\n",
    "        \n",
    "    documents:\n",
    "        The documents we will use to train each CountMinSketch.\n",
    "        /!\\ Should be the same that as been used for perfect_count.\n",
    "        \n",
    "    type:\n",
    "        Error function used, can be 'RMSE' or 'ARE'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def rmse_error(x, y):\n",
    "        return (x - y) ** 2\n",
    "    \n",
    "    def are_error(x, y):\n",
    "        return ((x - y) / y) ** 2\n",
    "        \n",
    "    # We will start by creating all the combinations for d & w.\n",
    "    for d in d_batch:\n",
    "        for w in w_batch:\n",
    "            # We train our counter\n",
    "            count_min = CountMinSketch(w, d)\n",
    "            count_min\n",
    "            \n",
    "        \n",
    "    for word in perfect_count:\n",
    "        real_count = perfect_count[word]\n",
    "        approx_count = counter_object.count(word)\n",
    "        error = \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare two words together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_words(CounterObject, word_a, word_b):\n",
    "    \"\"\"Compares two words and print the comparison on stdout.\"\"\"\n",
    "    \n",
    "    print('Comparing words `{}\\' and `{}\\'...'.format(word_a, word_b))\n",
    "    nb_occ_a = count_min.count(word_a)\n",
    "    nb_occ_b = count_min.count(word_b)\n",
    "    \n",
    "    most_used = None\n",
    "    less_used = None\n",
    "    factor = 0\n",
    "    if nb_occ_a > nb_occ_b:\n",
    "        nb_occ_b = 1 if nb_occ_b == 0 else nb_occ_b # This is gross, but it allows to avoid infinite values.\n",
    "        most_used = word_a\n",
    "        less_used = word_b\n",
    "        factor = nb_occ_a / nb_occ_b\n",
    "    else:\n",
    "        nb_occ_a = 1 if nb_occ_a == 0 else nb_occ_a # This is gross, but it allows to avoid infinite values.\n",
    "        most_used = word_b\n",
    "        less_used = word_a\n",
    "        factor = nb_occ_b / nb_occ_a\n",
    "    \n",
    "    print('Word `{}\\' is {:.2f} times more present than word `{}\\''.format(most_used, factor, less_used))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We stores in the perfect_count variable the real count of each word.\n",
    "# It is cached and will later be used for error checking.\n",
    "\n",
    "perfect_count = compute_perfect_count(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Basic Count-Min Sketch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountMinSketch:\n",
    "    \n",
    "    @staticmethod\n",
    "    def __build_seeds(nb):\n",
    "        \"\"\"Constructs an array of `nb' hash functions.\"\"\"\n",
    "        \n",
    "        # Generates random unique seeds first.\n",
    "        MIN_SEED = 1\n",
    "        MAX_SEED = 1000\n",
    "        seeds = random.sample(range(MIN_SEED, MAX_SEED), nb)        \n",
    "        return seeds\n",
    "    \n",
    "    def __init__(self, w, d):\n",
    "        self.__w = w;\n",
    "        self.__nb_hash = d;\n",
    "        self.__seeds = CountMinSketch.__build_seeds(self.__nb_hash)\n",
    "        self.__count = [[0 for x in range(self.__w)] for y in range(self.__nb_hash)]\n",
    "    \n",
    "    def __get_count_by_hash(self, word, hash_id):\n",
    "        \"\"\"Returns the count of a word for a given seed id.\"\"\"\n",
    "        \n",
    "        seed = self.__seeds[hash_id]\n",
    "        elt_idx = mmh3.hash(word, seed) % self.__w\n",
    "        \n",
    "        return self.__count[hash_id][elt_idx]\n",
    "    \n",
    "    def count(self, word):\n",
    "        \"\"\"Returns the estimated count of the given word.\"\"\"\n",
    "            \n",
    "        # Retrieves the count for each hash\n",
    "        counts = [self.__get_count_by_hash(word, i) for i in range(0, self.__nb_hash)]\n",
    "        return min(counts)\n",
    "    \n",
    "    def update_word(self, word):\n",
    "        \"\"\"Updates the CountMinSketch structure with a single word.\"\"\"\n",
    "        \n",
    "        # Loops over every hash function\n",
    "        for i in range(0, self.__nb_hash):\n",
    "            s = self.__seeds[i]\n",
    "            # Computes the index of the word for this particular hash\n",
    "            idx = mmh3.hash(word, s) % self.__w\n",
    "            # Updates the table containing the total count\n",
    "            self.__count[i][idx] = self.__count[i][idx] + 1\n",
    "            \n",
    "    def update(self, documents):\n",
    "        \"\"\"Updates the CountMinSketch structure with a batch of documents.\"\"\"\n",
    "        \n",
    "        # Loops over every provided documents\n",
    "        for doc in documents:\n",
    "            # Loops over every word\n",
    "            for word in doc:\n",
    "                self.update_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing words `the' and `direction'...\n",
      "Word `the' is 9.11 times more present than word `direction'\n",
      "\n",
      "Comparing words `the' and `a'...\n",
      "Word `the' is 1.98 times more present than word `a'\n"
     ]
    }
   ],
   "source": [
    "nb_hash_func = 5\n",
    "count_min_width = 100\n",
    "\n",
    "count_min = CountMinSketch(count_min_width, nb_hash_func)\n",
    "count_min.update(documents)\n",
    "\n",
    "compare_words(count_min, \"the\", \"direction\")\n",
    "print()\n",
    "compare_words(count_min, \"the\", \"a\")\n",
    "\n",
    "# As we may thought, the word `the' is a lot more present than the word 'direction`.\n",
    "# However, when checking the comparison between `the' and `a', we have a smaller difference of count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing words `the' and `direction'...\n",
      "Word `the' is 6.51 times more present than word `direction'\n",
      "\n",
      "Comparing words `the' and `a'...\n",
      "Word `the' is 1.74 times more present than word `a'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
